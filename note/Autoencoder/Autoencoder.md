# 摘要
自编码器是神经网络的一种特殊类型，其主要目的是将输入编码成压缩的、有意义的表示，然后再解码，使重构后的输入尽可能与原始输入相似。

输入是x，输出也是x，只不过中间进行的步骤是将内容折叠后再打开

# 组成结构

**Autoencoder**由编码器（Encoder）和解码器（Decoder）两部分组成。其基本架构如下：

- **编码器（Encoder）**：将高维输入数据映射到一个低维的潜在空间表示。
- **解码器（Decoder）**：从潜在空间表示重构出原始数据。


![[Pasted image 20240701163423.png]]

中间的隐藏层加入限制：维度较低

如果只是使用前半部分就可以得到低纬度的x的特征向量，即降维
而只是利用后面的部分就可以实现生维的作用

# 作用
自编码器主要用于无监督学习，即不需要标签的数据学习。其主要应用场景包括：

- **降维和特征提取**：通过压缩输入数据来提取有意义的低维特征表示。
- **数据去噪**：通过学习去除数据中的噪声来获得更清晰的表示。
- **异常检测**：通过比较输入数据和重构数据之间的差异来检测异常样本。




#  使用方式

自编码器的训练过程包括以下步骤：

1. **前向传播**：输入数据通过编码器转换为潜在表示，再通过解码器重构为输出数据。
2. **计算损失函数**：常用的损失函数包括均方误差（MSE）和二元交叉熵（Binary Cross-Entropy），用于衡量输入和重构输出之间的差异。
3. **反向传播**：通过反向传播算法更新网络参数，以最小化损失函数。

# 作用效果

自编码器能够在以下几个方面发挥作用：

- **特征学习**：通过无监督学习从原始数据中提取重要特征。
- **数据重构**：有效地重构输入数据，保留原始数据的主要信息。
- **去噪能力**：在数据去噪任务中表现良好，能够去除输入数据中的噪声。

# 类型
## undercomplete autoencoder

中间隐藏层维度降低，也就是under complete 不完整

生成出来的x和原来的进行mse得到loss函数

最小化loss函数就可以

![[img/Pasted image 20240705150915.png]]

## regularized autoencoder

### sparse autoencoder

正则化限制
比如，让隐藏层向量保持稀疏，指的是一大部分向量在很大程度上都是0

在这种情况下，每一次输入和中间维度都会发生变化，如果中间维度较高，那么就会在输入之前将期间的一部分设置为0，这种情况也被称之为overcomplete

这个叫做稀疏自动编码器

会在刚在loss函数基础上多添加一项：衡量中间层的特征向量的稀疏指标

$loss_{pixel}+loss_{sparse}$

#### 怎么保持稀疏
可以使用l1损失函数

$loss_{sparse} = \frac{1}{C}\sum_{i=1}^c|h_i|$

$h_i$这里指的是中间层的特征向量

## denoising autoencoder
图片出现噪音or缺失，需要降噪或者补充

