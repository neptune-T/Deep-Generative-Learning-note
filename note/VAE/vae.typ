#import "@preview/grape-suite:1.0.0": seminar-paper, german-dates

#set text(lang: "de")

#show: seminar-paper.project.with(
    title: "An Introduction to Variational Autoencoders",
    subtitle: "Notes to get started",

    university: [University of Chinese Academy of Sciences],

    semester: german-dates.semester(datetime.today()),

    author: "ZTS",
    email: "plote5024@mail.com",
    address: [
        Beijing
    ]
)

= Introduction

== Motivation
#v(.5em)
æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªä¸»è¦åˆ†æ”¯æ˜¯ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ã€‚åœ¨åˆ¤åˆ«å»ºæ¨¡ä¸­ï¼Œç›®æ ‡æ˜¯å­¦ä¹ ç»™å®šè§‚å¯Ÿå€¼çš„é¢„æµ‹å™¨ï¼Œè€Œåœ¨ç”Ÿæˆå»ºæ¨¡ä¸­ï¼Œç›®æ ‡æ˜¯è§£å†³å­¦ä¹ æ‰€æœ‰å˜é‡*All variables*çš„è”åˆåˆ†å¸ƒã€‚

ç”Ÿæˆæ¨¡å‹æ¨¡æ‹Ÿæ•°æ®åœ¨ç°å®ä¸–ç•Œä¸­çš„ç”Ÿæˆæ–¹å¼ã€‚åœ¨å‡ ä¹æ¯ä¸€é—¨ç§‘å­¦ä¸­ï¼Œâ€œå»ºæ¨¡â€éƒ½è¢«ç†è§£ä¸ºé€šè¿‡å‡è®¾ç†è®ºå’Œé€šè¿‡è§‚å¯Ÿæµ‹è¯•è¿™äº›ç†è®ºæ¥æ­ç¤ºè¿™ä¸ªç”Ÿæˆè¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œå½“æ°”è±¡å­¦å®¶ä¸ºå¤©æ°”å»ºæ¨¡æ—¶ï¼Œä»–ä»¬ä½¿ç”¨é«˜åº¦å¤æ‚çš„åå¾®åˆ†æ–¹ç¨‹æ¥è¡¨è¾¾å¤©æ°”çš„æ½œåœ¨ç‰©ç†ç‰¹æ€§ã€‚ç”Ÿç‰©å­¦å®¶ã€åŒ–å­¦å®¶ã€ç»æµå­¦å®¶ç­‰ç­‰ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç§‘å­¦ä¸­çš„å»ºæ¨¡å®é™…ä¸Šå‡ ä¹å…¨æ˜¯ç”Ÿæˆæ¨¡å‹ã€‚

è¯•å›¾ç†è§£æ•°æ®ç”Ÿæˆè¿‡ç¨‹çš„å¦ä¸€ä¸ªåŸå› æ˜¯ï¼Œå®ƒè‡ªç„¶åœ°è¡¨è¾¾äº†ä¸–ç•Œçš„å› æœå…³ç³»ã€‚å› æœå…³ç³»çš„ä¼˜ç‚¹ï¼Œå°±æ˜¯å®ƒä»¬æ¯”å•çº¯çš„ç›¸å…³æ€§æ›´èƒ½æ¦‚æ‹¬æ–°æƒ…å†µã€‚

ä¸ºäº†å°†ç”Ÿæˆæ¨¡å‹è½¬åŒ–ä¸ºDiscriminatorï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨è´å¶æ–¯è§„åˆ™ã€‚å³æˆ‘ä»¬å¯ä»¥å°†ç”Ÿæˆæ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºåˆ†ç±»ä»»åŠ¡ä¸­éœ€è¦çš„æ¡ä»¶æ¦‚ç‡ï¼Œä»è€Œå®ç°åˆ¤åˆ«åŠŸèƒ½ã€‚

åœ¨åˆ¤åˆ«æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ç›´æ¥å­¦ä¹ ä¸€ä¸ªä¸æœªæ¥é¢„æµ‹æ–¹å‘ä¸€è‡´çš„æ˜ å°„ã€‚è¿™ä¸ç”Ÿæˆæ¨¡å‹çš„æ–¹å‘ç›¸åã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è®¤ä¸ºä¸€å¼ å›¾åƒåœ¨ç°å®ä¸–ç•Œä¸­æ˜¯é€šè¿‡å…ˆè¯†åˆ«ç‰©ä½“ï¼Œç„¶åç”Ÿæˆä¸‰ç»´ç‰©ä½“ï¼Œå†å°†å…¶æŠ•å½±åˆ°åƒç´ ç½‘æ ¼ä¸Šæ¥ç”Ÿæˆçš„ã€‚

è€Œåˆ¤åˆ«æ¨¡å‹åˆ™ç›´æ¥å°†è¿™äº›åƒç´ å€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°æ ‡ç­¾ä¸Šã€‚è™½ç„¶ç”Ÿæˆæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆåœ°ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œä½†å®ƒä»¬å¾€å¾€æ¯”çº¯ç²¹çš„åˆ¤åˆ«æ¨¡å‹å¯¹æ•°æ®åšå‡ºæ›´å¼ºçš„å‡è®¾ï¼Œè¿™é€šå¸¸ä¼šåœ¨æ¨¡å‹å‡ºé”™æ—¶å¯¼è‡´æ›´é«˜çš„æ¸è¿‘åå·®ï¼ˆ*Asymptotic bias*ï¼‰ã€‚

å› æ­¤ï¼Œå¦‚æœæ¨¡å‹å‡ºé”™ï¼ˆäº‹å®ä¸Šå‡ ä¹æ€»æ˜¯æœ‰ä¸€å®šç¨‹åº¦çš„è¯¯å·®ï¼‰ï¼Œå¦‚æœæˆ‘ä»¬åªå…³æ³¨äºå­¦ä¹ å¦‚ä½•åŒºåˆ†ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰è¶³å¤Ÿå¤šçš„æ•°æ®ï¼Œé‚£ä¹ˆçº¯ç²¹çš„åˆ¤åˆ«æ¨¡å‹åœ¨åˆ¤åˆ«ä»»åŠ¡ä¸­é€šå¸¸ä¼šå¯¼è‡´æ›´å°‘çš„é”™è¯¯ã€‚ç„¶è€Œï¼Œå–å†³äºæ•°æ®é‡çš„å¤šå°‘ï¼Œç ”ç©¶æ•°æ®ç”Ÿæˆè¿‡ç¨‹å¯èƒ½æœ‰åŠ©äºæŒ‡å¯¼åˆ¤åˆ«å™¨ï¼ˆå¦‚åˆ†ç±»å™¨ï¼‰çš„è®­ç»ƒã€‚ä¾‹å¦‚ï¼Œåœ¨åŠç›‘ç£å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½åªæœ‰å°‘é‡çš„æ ‡è®°æ ·æœ¬ï¼Œä½†æœ‰æ›´å¤šçš„æœªæ ‡è®°æ ·æœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯ä»¥åˆ©ç”¨æ•°æ®çš„ç”Ÿæˆæ¨¡å‹æ¥æ”¹è¿›åˆ†ç±»ã€‚

è¿™ç§æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ„å»ºæœ‰ç”¨çš„ä¸–ç•ŒæŠ½è±¡ï¼Œè¿™äº›æŠ½è±¡å¯ä»¥ç”¨äºå¤šä¸ªåç»­çš„é¢„æµ‹ä»»åŠ¡ã€‚è¿™ç§è¿½æ±‚æ•°æ®ä¸­è§£ç¼ ã€è¯­ä¹‰ä¸Šæœ‰æ„ä¹‰ã€ç»Ÿè®¡ä¸Šç‹¬ç«‹å’Œå› æœå…³ç³»çš„å˜åŒ–å› ç´ çš„è¿‡ç¨‹ï¼Œé€šå¸¸è¢«ç§°ä¸ºæ— ç›‘ç£è¡¨ç¤ºå­¦ä¹ ï¼Œè€Œå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å·²ç»å¹¿æ³›åº”ç”¨äºæ­¤ç›®çš„ã€‚

æˆ–è€…ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºä¸€ç§éšå¼æ­£åˆ™åŒ–å½¢å¼ï¼šé€šè¿‡å¼ºåˆ¶è¡¨ç¤ºå¯¹æ•°æ®ç”Ÿæˆè¿‡ç¨‹æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬å°†ä»è¾“å…¥åˆ°è¡¨ç¤ºçš„æ˜ å°„è¿‡ç¨‹çº¦æŸåœ¨æŸç§ç‰¹å®šçš„æ¨¡å¼ä¸­ã€‚é€šè¿‡é¢„æµ‹ä¸–ç•Œè¿™ä¸€è¾…åŠ©ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æŠ½è±¡å±‚é¢ä¸Šæ›´å¥½åœ°ç†è§£ä¸–ç•Œï¼Œä»è€Œæ›´å¥½åœ°è¿›è¡Œåç»­çš„é¢„æµ‹ã€‚

å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰å¯ä»¥çœ‹ä½œæ˜¯ä¸¤ä¸ªè€¦åˆä½†ç‹¬ç«‹å‚æ•°åŒ–çš„æ¨¡å‹ï¼šç¼–ç å™¨æˆ–è¯†åˆ«æ¨¡å‹*(Encoder or Recognition Model)*ï¼Œä»¥åŠè§£ç å™¨æˆ–ç”Ÿæˆæ¨¡å‹*(Decoder or Generative Model)*ã€‚è¿™ä¸¤ä¸ªæ¨¡å‹ç›¸äº’æ”¯æŒã€‚è¯†åˆ«æ¨¡å‹å‘ç”Ÿæˆæ¨¡å‹æä¾›å…¶åéªŒåˆ†å¸ƒçš„è¿‘ä¼¼å€¼ï¼Œåè€…éœ€è¦è¿™äº›è¿‘ä¼¼å€¼åœ¨â€œæœŸæœ›æœ€å¤§åŒ–â€å­¦ä¹ çš„è¿­ä»£è¿‡ç¨‹ä¸­æ›´æ–°å…¶å‚æ•°ã€‚åè¿‡æ¥ï¼Œç”Ÿæˆæ¨¡å‹ä¸ºè¯†åˆ«æ¨¡å‹æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„æœ‰æ„ä¹‰è¡¨ç¤ºï¼ŒåŒ…æ‹¬å¯èƒ½çš„ç±»åˆ«æ ‡ç­¾ã€‚æ ¹æ®è´å¶æ–¯è§„åˆ™ï¼Œè¯†åˆ«æ¨¡å‹æ˜¯ç”Ÿæˆæ¨¡å‹çš„è¿‘ä¼¼é€†ã€‚

ä¸æ™®é€šçš„å˜åˆ†æ¨æ–­ï¼ˆVIï¼‰ç›¸æ¯”ï¼ŒVAEæ¡†æ¶çš„ä¸€ä¸ªä¼˜åŠ¿åœ¨äºè¯†åˆ«æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºæ¨æ–­æ¨¡å‹ï¼‰ç°åœ¨æ˜¯è¾“å…¥å˜é‡çš„ä¸€ä¸ªï¼ˆéšæœºï¼‰å‡½æ•°ã€‚è¿™ä¸VIä¸åŒï¼Œåè€…å¯¹æ¯ä¸ªæ•°æ®å®ä¾‹éƒ½æœ‰ä¸€ä¸ªç‹¬ç«‹çš„å˜åˆ†åˆ†å¸ƒï¼Œè¿™å¯¹äºå¤§æ•°æ®é›†æ¥è¯´æ•ˆç‡ä½ä¸‹ã€‚è¯†åˆ«æ¨¡å‹ä½¿ç”¨ä¸€ç»„å‚æ•°æ¥å»ºæ¨¡è¾“å…¥ä¸æ½œå˜é‡ä¹‹é—´çš„å…³ç³»ï¼Œå› æ­¤è¢«ç§°ä¸ºâ€œæ‘Šé”€æ¨æ–­â€ã€‚è¿™ç§è¯†åˆ«æ¨¡å‹å¯ä»¥æ˜¯ä»»æ„å¤æ‚çš„ï¼Œä½†ç”±äºå…¶æ„é€ æ–¹å¼ï¼Œåªéœ€ä¸€æ¬¡ä»è¾“å…¥åˆ°æ½œå˜é‡çš„å‰é¦ˆä¼ é€’å³å¯å®Œæˆï¼Œå› æ­¤ä»ç„¶ç›¸å¯¹å¿«é€Ÿã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»˜å‡ºçš„ä»£ä»·æ˜¯ï¼Œè¿™ç§é‡‡æ ·ä¼šåœ¨å­¦ä¹ æ‰€éœ€çš„æ¢¯åº¦ä¸­å¼•å…¥é‡‡æ ·å™ªå£°ã€‚

æˆ–è®¸VAEæ¡†æ¶æœ€å¤§çš„è´¡çŒ®æ˜¯è®¤è¯†åˆ°æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°åœ¨è¢«ç§°ä¸ºâ€œé‡å‚æ•°åŒ–æŠ€å·§â€çš„ç®€å•æ–¹æ³•æ¥é‡æ–°ç»„ç»‡æˆ‘ä»¬çš„æ¢¯åº¦è®¡ç®—ï¼Œä»è€Œå‡å°‘æ¢¯åº¦ä¸­çš„æ–¹å·®ã€‚

== Aim
#v(.5em)
*provides a principled method for jointly learning _deep latent-variable models_ and corresponding inference models using _stochastic gradient descent_.*

è¯¥æ¡†æ¶åœ¨ç”Ÿæˆå»ºæ¨¡ã€åŠç›‘ç£å­¦ä¹ å’Œè¡¨ç¤ºå­¦ä¹ ç­‰æ–¹é¢æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚

== Probabilistic Models and Variational Inference
#v(.5em)
ç”±äºæ¦‚ç‡æ¨¡å‹åŒ…å«æœªçŸ¥æ•°ä¸”æ•°æ®å¾ˆå°‘èƒ½å®Œæ•´åœ°æè¿°è¿™äº›æœªçŸ¥å‚æ•°ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦å¯¹æ¨¡å‹çš„æŸäº›æ–¹é¢å‡è®¾ä¸€å®šç¨‹åº¦çš„ä¸ç¡®å®šæ€§ã€‚è¿™ç§ä¸ç¡®å®šæ€§çš„ç¨‹åº¦å’Œæ€§è´¨é€šè¿‡ï¼ˆæ¡ä»¶ï¼‰æ¦‚ç‡åˆ†å¸ƒæ¥æè¿°ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œæœ€å®Œæ•´çš„æ¦‚ç‡æ¨¡å‹å½¢å¼é€šè¿‡è¿™äº›å˜é‡çš„è”åˆæ¦‚ç‡åˆ†å¸ƒæ¥æŒ‡å®šæ¨¡å‹ä¸­æ‰€æœ‰å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§å’Œé«˜é˜¶ä¾èµ–å…³ç³»ã€‚

æˆ‘ä»¬ç”¨ $bold(x)$ è¡¨ç¤ºæ‰€æœ‰è§‚æµ‹å˜é‡çš„å‘é‡ï¼Œå…¶è”åˆåˆ†å¸ƒæ˜¯æˆ‘ä»¬å¸Œæœ›å»ºæ¨¡çš„ã€‚ï¼ˆä½¿ç”¨å°å†™ç²—ä½“æ¥è¡¨ç¤ºåº•å±‚çš„è§‚æµ‹éšæœºå˜é‡é›†ï¼Œå³å°†å…¶å±•å¹³å’Œæ‹¼æ¥ï¼Œä½¿è¯¥é›†åˆè¡¨ç¤ºä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡ã€‚ï¼‰

å‡è®¾è§‚æµ‹å˜é‡ $bold(x)$ æ˜¯æ¥è‡ªæœªçŸ¥åº•å±‚è¿‡ç¨‹çš„éšæœºæ ·æœ¬ï¼Œå…¶çœŸå®çš„æ¦‚ç‡åˆ†å¸ƒ $p^*(bold(x)) $ æ˜¯æœªçŸ¥çš„ã€‚æˆ‘ä»¬å°è¯•ç”¨ä¸€ä¸ªé€‰å®šçš„æ¨¡å‹ $p_theta bold(x)$ æ¥è¿‘ä¼¼è¿™ä¸ªåº•å±‚è¿‡ç¨‹ï¼Œå…¶ä¸­å‚æ•°ä¸º$theta$ :
$
bold(x) tilde p_theta (bold(x))
$
å­¦ä¹ æœ€å¸¸è§çš„æ˜¯ä¸€ä¸ªæœç´¢å‚æ•° $theta$ çš„è¿‡ç¨‹ï¼Œä½¿å¾—ç”±æ¨¡å‹ç»™å‡ºçš„æ¦‚ç‡åˆ†å¸ƒå‡½æ•° $p_theta (bold(x)) $ è¿‘ä¼¼äºæ•°æ®çš„çœŸå®åˆ†å¸ƒ $p^*(bold(x)) $ï¼Œå³å¯¹äºä»»ä½•è§‚æµ‹åˆ°çš„ $bold(x)$ï¼Œä¸¤è€…å°½å¯èƒ½æ¥è¿‘:
$
p_theta (bold(x)) approx p^* (bold(x))
$

=== Conditional Models
åœ¨åˆ†ç±»æˆ–è€…å›å½’é—®é¢˜ä¸Šé¢ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒæ— æ¡ä»¶æ¨¡å‹ $p_theta (bold(x))$ï¼Œæ›´å€¾å‘äºæ¡ä»¶æ¨¡å‹ $p_theta (y|x)$ï¼Œå®ƒè¿‘ä¼¼äºåº•å±‚çš„æ¡ä»¶åˆ†å¸ƒ $p^*(y|x)$ï¼šå³åœ¨è§‚æµ‹å˜é‡$bold(x)$çš„å€¼ä¸Šï¼Œå¯¹å˜é‡$y$çš„å€¼è¿›è¡Œåˆ†å¸ƒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œbold(x)é€šå¸¸è¢«ç§°ä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚ä¸æ— æ¡ä»¶æƒ…å†µç±»ä¼¼ï¼Œæˆ‘ä»¬é€‰æ‹©å¹¶ä¼˜åŒ–ä¸€ä¸ªæ¨¡å‹ $p_theta (y|x)$ï¼Œä½¿å…¶æ¥è¿‘æœªçŸ¥çš„åº•å±‚åˆ†å¸ƒï¼Œå³å¯¹äºä»»ä½• $bold(x) $ å’Œ $y$:

$
p_(theta)(y|bold(x)) approx p^*(y|bold(x))
$

== Directed Graphical Models and Neural Networks
#v(.9em)
æˆ‘ä»¬ä½¿ç”¨æœ‰å‘æ¦‚ç‡å›¾æ¨¡å‹(Directed probability graph model)ï¼Œæˆ–è´å¶æ–¯ç½‘ç»œã€‚æœ‰å‘å›¾æ¨¡å‹æ˜¯ä¸€ç§æ¦‚ç‡æ¨¡å‹ï¼Œå…¶ä¸­æ‰€æœ‰çš„å˜é‡è¢«æ‹“æ‰‘ç»„ç»‡æˆä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€‚è¿™äº›æ¨¡å‹çš„å˜é‡çš„è”åˆåˆ†å¸ƒè¢«åˆ†è§£ä¸ºå…ˆéªŒåˆ†å¸ƒå’Œæ¡ä»¶åˆ†å¸ƒçš„ä¹˜ç§¯:
$
p_(theta) (bold(X)_1,...,bold(X)_M) = product_(j=1)^M p_(theta)(bold(X)_j | P_a (bold(X)_j))
$
$P_a (bold(x)_j)$ æ˜¯æœ‰å‘å›¾ä¸­èŠ‚ç‚¹ $j$ çš„çˆ¶å‘é‡çš„é›†åˆã€‚

== Dataset
#v(.9em)
æˆ‘ä»¬æ”¶é›†$Nâ‰¥1$ä¸ªæ•°æ®ç‚¹ç»„æˆçš„æ•°æ®é›† $cal(D)$:
$
cal(D) = {bold(x)^((1)),bold(x)^((2)),...,bold(x)^((N))} equiv {bold(x^((i)))}_(i=1)^N equiv bold(x)^((1:N))
$

æ•°æ®é›†è¢«è®¤ä¸ºç”±åŒä¸€ï¼ˆä¸å˜ï¼‰ç³»ç»Ÿçš„ç‹¬ç«‹æµ‹é‡å€¼ç»„æˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè§‚æµ‹æ•°æ® $cal(D) = { bold(x)^((i))}_(i=1)^N$è¢«ç§°ä¸ºç‹¬ç«‹åŒåˆ†å¸ƒ(i.i.d.)ã€‚åœ¨ç‹¬ç«‹åŒåˆ†å¸ƒçš„å‡è®¾ä¸‹ï¼Œç»™å®šå‚æ•°çš„æƒ…å†µä¸‹ï¼Œæ•°æ®ç‚¹çš„æ¦‚ç‡å› å­åŒ–ä¸ºå„ä¸ªæ•°æ®ç‚¹æ¦‚ç‡çš„ä¹˜ç§¯ã€‚å› æ­¤ï¼Œæ¨¡å‹åˆ†é…ç»™æ•°æ®çš„å¯¹æ•°æ¦‚ç‡è¡¨ç¤ºä¸ºï¼š
$ 
log p_theta(cal(D)) = sum_(x in cal(D)) log p_theta(x) $
== Maximum Likelihood and Minibatch SGD
#v(.9em)
æ¦‚ç‡æ¨¡å‹æœ€å¸¸è§çš„æ ‡å‡†æ˜¯æœ€å¤§å¯¹æ•°ä¼¼ç„¶(ML). *Maximization of the log-likelihood criterion is equivalent to minimization of a Kullback Leibler divergence between the data and model distributions.*

ä¼˜åŒ–æ–¹å¼æ¯”è¾ƒå¥½ç”¨çš„æ–¹æ³•æ˜¯*_stochastic gradient descent_*

è€ƒè™‘ä¸€ä¸ªæ•°æ®é›†$cal(D)$åŒ…å«Nä¸ªæ•°æ®ç‚¹,æˆ‘ä»¬æ— æ³•æ¯æ¬¡éƒ½ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ¥è®¡ç®—æ¢¯åº¦ï¼Œå› ä¸ºè¿™æ ·è®¡ç®—ä»£ä»·å¤ªé«˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡æ•°æ® $cal(M)$ï¼ˆä¾‹å¦‚ï¼Œå¤§å°ä¸º$N_cal(M)$ï¼‰ï¼Œå¹¶è®¡ç®—è¿™ä¸ªå°æ‰¹é‡æ•°æ®ä¸Šçš„æ¢¯åº¦ã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå°æ‰¹é‡æ•°æ®ä¸Šçš„æ¢¯åº¦èƒ½å¤Ÿåœ¨æœŸæœ›å€¼ä¸Šç­‰äºæ•´ä¸ªæ•°æ®é›†ä¸Šçš„æ¢¯åº¦ã€‚

åœ¨éšæœºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œè¿™æ„å‘³ç€ï¼ˆå…¬å¼æ¨å¯¼æ”¾åœ¨1.6.1ï¼Œä¸å ç”¨ç¯‡å¹…ç»§ç»­å†™ç»“è®ºï¼‰ï¼š

$
EE[nabla_theta log p_theta (cal(M))] = nabla_theta log p_theta (cal(D))
$
$cal(M)$æ˜¯ä»$cal(D)$ä¸­éšæœºæŠ½å‡ºæ¥çš„å°æ‰¹é‡æ•°æ®ï¼Œ$log p_theta (cal(M))$æ˜¯è¿™ä¸ªå°æ‰¹é‡çš„å¯¹æ•°ä¼¼ç„¶ï¼Œ$log p_theta (cal(D))$æ˜¯æ•´ä¸ªæ•°æ®é›†çš„å¯¹æ•°ä¼¼ç„¶ã€‚

å¯¹æ•°ä¼¼ç„¶æ— åä¼°è®¡ï¼š

å¯¹äºæ•´ä¸ªæ•°æ®é›† $cal(D)$ï¼Œå…¶å¯¹æ•°ä¼¼ç„¶æ˜¯ï¼š
$
log p_theta (cal(D)) = sum_(x in cal(D)) log p_theta (x)
$
å¯¹äºå°æ‰¹é‡æ•°æ® $cal(M)$ï¼Œå…¶å¯¹æ•°ä¼¼ç„¶æ˜¯ï¼š
$
log p_theta (cal(M)) = sum_(x in cal(M)) log p_theta (x)
$
åœ¨æœŸæœ›ä¸Šï¼Œå°æ‰¹é‡æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶å¯ä»¥è¿‘ä¼¼æ•´ä¸ªæ•°æ®é›†çš„å¯¹æ•°ä¼¼ç„¶ï¼š
$
1/N_cal(D) log p_theta (cal(D)) tilde.eq 1/N_cal(M) log p_theta (cal(M)) 
$

é€šè¿‡è¿™æ ·çš„è¿·ä½ æ‰¹æ¬¡*_minibatches_*ï¼Œæˆ‘ä»¬å¯ä»¥å½¢æˆæœ€å¤§ä¼¼ç„¶å‡†åˆ™çš„æ— åä¼°è®¡ï¼š
$
1/N_cal(D) log p_theta (cal(D)) tilde.eq 1/N_cal(M) log p_theta (cal(M)) = 1/N_cal(M) sum_(x in cal(M)) log p_theta (x)
$

$
frac(1, N_cal(D)) nabla_theta log p_theta (cal(D)) tilde.eq frac(1, N_cal(M)) nabla_theta log p_theta (cal(M)) = frac(1, N_cal(M)) sum _ (x in cal(M)) nabla_theta log p_theta (x)
$

=== å°æ‰¹é‡æ•°æ®çš„æ¢¯åº¦æœŸæœ›å€¼ç­‰äºæ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦
#v(.9em)
å‡è®¾æ•°æ®é›†$cal(D)$æœ‰$N$ä¸ªæ•°æ®é›†ï¼Œå°æ‰¹é‡$cal(M)$ä¹‹ä¸­æœ‰$N_cal(M)$ä¸ªæ•°æ®é›†ï¼Œå¹¶ä¸”å°æ‰¹é‡æ•°æ®æ˜¯ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–çš„ã€‚

æ•´ä¸ªæ•°æ®é›†çš„å¯¹æ•°ä¼¼ç„¶æ¢¯åº¦:
$
nabla_theta log p_theta (cal(D)) = nabla_theta sum_(i=1) ^N log p_theta (x_i) = sum_(i=1) ^N nabla_theta log p_theta (x_i)
$

å°æ‰¹é‡æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶æ¢¯åº¦ï¼š
$
nabla_theta log p_theta (cal(M)) = N/N_cal(M) sum_(j=1)^N_cal(M) nabla_theta log p_theta(x_j)
$
è¿™é‡Œä¹˜ä»¥$N/N_cal(M)$è¯æ˜å†™åœ¨1.6.1.1äº†ï¼Œè¿™é‡Œå°±ä¸å ç¯‡å¹…ç»§ç»­å‘ä¸‹æ¨å¯¼äº†ã€‚

å‡è®¾æˆ‘ä»¬æŠ½å–äº†$N_cal(M)$ä¸ªæ ·æœ¬ï¼Œæ ·æœ¬æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ•°å­¦æœŸæœ›æ¥è¡¨ç¤ºè¿™ç§æŠ½æ ·è¿‡ç¨‹ï¼š

$
EE[nabla_theta log p_theta (cal(M))] = EE [N/N_cal(M) sum _(j=1)^N_(cal(M)) nabla_theta log p_theta (x_j)]
$

ç”±äº $cal(M)$ ä¸­çš„æ ·æœ¬æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæ‰€ä»¥:

$
EE[sum_(j=1)^(N_cal(M)) nabla_theta log p_theta (x_j)] = N_cal(M) EE [nabla_theta log p_theta (x)]
$

å…¶ä¸­$x$æ˜¯ä»$cal(D)$ä¸­éšæœºæŠ½å–çš„ä¸€ä¸ªæ ·æœ¬ã€‚å› æ­¤:

$
EE[nabla_theta log p_theta (cal(M))] 
    &= EE [N/N_cal(M) sum _(j=1)^N_(cal(M)) nabla_theta log p_theta (x_j)]\
    &= N/N_cal(M) N_cal(M) EE[nabla_theta log p_theta (x)]\
    &= N EE [nabla_theta log p_theta (x)]
$
ç”±äºæ•°æ®ç‚¹ $x$ æ˜¯ä»æ•´ä¸ªæ•°æ®é›†ä¸­æŠ½å–çš„ï¼Œæ‰€ä»¥ $EE[nabla_theta log p_theta (x)]$ å®é™…ä¸Šå°±æ˜¯æ€»ä½“æ¢¯åº¦çš„å¹³å‡å€¼ï¼š
$
N EE [nabla_theta log p_theta (x)] = sum_(i=1)^N nabla_theta log p_theta (x_i) = nabla_theta log p_theta (cal(D))
$


==== Coefficient proof
#v(.9em)
æ˜¯ä¸ºäº†è¡¥å¿å°æ‰¹é‡æ•°æ®ä¸æ•´ä¸ªæ•°æ®é›†çš„å¤§å°å·®å¼‚ï¼Œä½¿å¾—æ¢¯åº¦ä¼°è®¡åœ¨æœŸæœ›å€¼ä¸Šä¿æŒä¸€è‡´ã€‚è¿™é‡Œä¹˜ç³»æ•°çš„åŸå› æ˜¯ï¼š

+ æ¯”ä¾‹è°ƒæ•´ï¼šç”±äºå°æ‰¹é‡æ•°æ® $cal(M)$ åªæ˜¯æ•´ä¸ªæ•°æ®é›† $cal(D)$ çš„ä¸€ä¸ªå­é›†ï¼Œå…¶æ ·æœ¬æ•°é‡ $N_cal(M)$ å°äº $N$ã€‚ä¹˜ä»¥æ¯”ä¾‹ç³»æ•°å¯ä»¥è°ƒæ•´å°æ‰¹é‡æ•°æ®çš„æ¢¯åº¦ä¼°è®¡ï¼Œä½¿å…¶åœ¨æœŸæœ›ä¸Šä¸æ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦ä¸€è‡´ã€‚
+ æ— åä¼°è®¡ï¼šè¿™ç§è°ƒæ•´ç¡®ä¿äº†å°æ‰¹é‡æ•°æ®æ¢¯åº¦ä¼°è®¡çš„æœŸæœ›å€¼ç­‰äºæ•´ä½“æ•°æ®é›†çš„æ¢¯åº¦ï¼Œä»è€Œåœ¨æœŸæœ›ä¸Šä¿æŒæ— åã€‚

å®šä¹‰æ•´ä¸ªæ•°æ®é›†çš„å¯¹æ•°ä¼¼ç„¶æ¢¯åº¦ï¼š
$
nabla_theta log p_theta (cal(D)) = nabla_theta sum_(i=1) ^N log p_theta (x_i) = sum_(i=1) ^N nabla_theta log p_theta (x_i)
$
å®šä¹‰å°æ‰¹é‡æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶æ¢¯åº¦ï¼š
$
nabla_theta log p_theta (cal(M)) =  sum_(j=1)^N_cal(M) nabla_theta log p_theta (x_j)
$

ç°åœ¨å‡è®¾æˆ‘ä»¬ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å°æ‰¹é‡æ•°æ® $cal(M)$ ,ç”±äº $cal(M)$ ä¸­çš„æ•°æ®ç‚¹æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œæˆ‘ä»¬æœ‰:

$
EE [nabla_theta log p_theta (cal(M))] = EE[sum_(j=1)^N_cal(M) nabla_theta log p_theta (x_j)] = N_cal(M)EE [nabla_theta log p_theta (x)]
$
ä¸ºäº†ä½¿å°æ‰¹é‡æ•°æ®çš„æ¢¯åº¦åœ¨æœŸæœ›ä¸Šç­‰äºæ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦å°†å°æ‰¹é‡æ•°æ®çš„æ¢¯åº¦è¿›è¡Œæ¯”ä¾‹è°ƒæ•´ã€‚æˆ‘ä»¬å®šä¹‰è°ƒæ•´åçš„æ¢¯åº¦ä¼°è®¡ä¸ºï¼š

$
tilde(nabla)_theta log p_theta (cal(M)) = N/N_M sum_(j=1)^(N_M) nabla_theta log p_theta (x_j)
$

æˆ‘ä»¬ç°åœ¨è®¡ç®—è¿™ä¸ªè°ƒæ•´åçš„æ¢¯åº¦çš„æœŸæœ›å€¼ï¼š
$
bb(E)[tilde(nabla)_theta log p_theta(cal(M))] 
&= bb(E)[N/N_M sum_(j=1)^(N_M) nabla_theta log p_theta (x_j)]\
&= N/N_M bb(E)[sum_(j=1)^(N_M) nabla_theta log p_theta (x_j)] \
&= N/N_M N_M bb(E)[nabla_theta log p_theta (x)] \
&= N bb(E)[nabla_theta log p_theta (x)]
$
ç”±äº$x$æ˜¯ä»æ•´ä¸ªæ•°æ®é›†ä¸­éšæœºæŠ½å–çš„æ ·æœ¬ï¼Œæˆ‘ä»¬æœ‰:
$
N EE [nabla_theta log p_theta (x)] = sum_(i=1)^N nabla_theta log p_theta (x_i) = nabla_theta log p_theta (cal(D))
$

== Learning and Inference in Deep Latent Variable Models
#v(.9em)

=== Latent Variables
#v(.9em)
æˆ‘ä»¬å¯ä»¥å°†å‰ä¸€èŠ‚è®¨è®ºçš„å®Œå…¨è§‚æµ‹æœ‰å‘æ¨¡å‹æ‰©å±•åˆ°åŒ…å«æ½œå˜é‡çš„æœ‰å‘æ¨¡å‹ã€‚æ½œå˜é‡æ˜¯æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä½†æˆ‘ä»¬å¹¶ä¸è§‚æµ‹åˆ°å®ƒä»¬ï¼Œå› æ­¤å®ƒä»¬ä¸å±äºæ•°æ®é›†çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬é€šå¸¸ç”¨ $bold(z)$ æ¥è¡¨ç¤ºè¿™æ ·çš„æ½œå˜é‡ã€‚åœ¨æ— æ¡ä»¶å»ºæ¨¡è§‚æµ‹å˜é‡ $bold(x)$ çš„æƒ…å†µä¸‹ï¼Œæœ‰å‘å›¾æ¨¡å‹å°†è¡¨ç¤ºä¸€ä¸ªè§‚æµ‹å˜é‡ $bold(x)$ å’Œæ½œå˜é‡ $bold(z)$ çš„è”åˆåˆ†å¸ƒ $p_theta (bold(x)|bold(z))$ã€‚è§‚æµ‹å˜é‡ $bold(x)$ çš„è¾¹ç¼˜åˆ†å¸ƒç”±ä»¥ä¸‹å…¬å¼ç»™å‡ºï¼š
$
p_theta = integral p_theta (bold(x),bold(z)) dif bold(z)
$
è¿™ä¹Ÿè¢«ç§°ä¸ºï¼ˆå•ä¸ªæ•°æ®ç‚¹çš„ï¼‰è¾¹ç¼˜ä¼¼ç„¶æˆ–æ¨¡å‹è¯æ®ï¼Œå½“å®ƒä½œä¸º $theta$ çš„å‡½æ•°æ—¶ã€‚

è¿™ç§å…³äº $bold(x)$ çš„éšå¼åˆ†å¸ƒå¯èƒ½æ˜¯éå¸¸çµæ´»çš„ã€‚å¦‚æœ $bold(z)$ æ˜¯ç¦»æ•£çš„ï¼Œå¹¶ä¸” $p_theta = (bold(x)|bold(z))$ æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œé‚£ä¹ˆ $p_theta (bold(x))$ å°±æ˜¯ä¸€ä¸ªé«˜æ–¯æ··åˆåˆ†å¸ƒã€‚å¯¹äºè¿ç»­çš„ $bold(z)$,  $p_theta (bold(x))$ å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªæ— é™æ··åˆï¼Œè¿™ç§æ··åˆæ½œåœ¨åœ°æ¯”ç¦»æ•£æ··åˆæ›´å¼ºå¤§ã€‚è¿™æ ·çš„è¾¹ç¼˜åˆ†å¸ƒä¹Ÿç§°ä¸ºå¤åˆæ¦‚ç‡åˆ†å¸ƒã€‚

=== Deep Latent Variable Models
#v(.9em)
*_deep latent variable model_ (DLVM) *


è¡¨ç¤ºé‚£äº›åˆ†å¸ƒç”±ç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„æ½œå˜é‡æ¨¡å‹ $p_theta (bold(x), bold(z) )$ ã€‚è¿™æ ·çš„æ¨¡å‹å¯ä»¥åŸºäºæŸäº›ä¸Šä¸‹æ–‡æ¡ä»¶ï¼Œå¦‚  $p_theta (bold(x),bold(z) | bold(y))$ã€‚DLVM çš„ä¸€ä¸ªé‡è¦ä¼˜åŠ¿æ˜¯ï¼Œå³ä½¿æœ‰å‘æ¨¡å‹ä¸­çš„æ¯ä¸ªå› å­ï¼ˆå…ˆéªŒæˆ–æ¡ä»¶åˆ†å¸ƒï¼‰ç›¸å¯¹ç®€å•ï¼ˆä¾‹å¦‚æ¡ä»¶é«˜æ–¯åˆ†å¸ƒï¼‰ï¼Œå…¶è¾¹ç¼˜åˆ†å¸ƒ $p_theta (bold(x)) $ ä¹Ÿå¯ä»¥éå¸¸å¤æ‚ï¼Œå³åŒ…å«å‡ ä¹ä»»æ„çš„ä¾èµ–å…³ç³»ã€‚è¿™ç§è¡¨è¾¾èƒ½åŠ›ä½¿å¾—æ·±åº¦æ½œå˜é‡æ¨¡å‹åœ¨è¿‘ä¼¼å¤æ‚çš„åº•å±‚åˆ†å¸ƒ $ p^*(bold(x))$ æ—¶éå¸¸æœ‰å¸å¼•åŠ›ã€‚

æˆ–è®¸æœ€ç®€å•ä¹Ÿæ˜¯æœ€å¸¸è§çš„ DLVM æ˜¯é€šè¿‡ä»¥ä¸‹ç»“æ„æŒ‡å®šçš„åˆ†è§£æ¨¡å‹ï¼š

$ 
p_theta (bold(x), bold(z)) = p_theta (bold(z)) p_theta (bold(x)|bold(z)) 
$
å…¶ä¸­, $p_theta (bold(z))$ å’Œ $p_theta (bold(x)|bold(z) $ æ˜¯å·²çŸ¥çš„ã€‚åˆ†å¸ƒ $p(bold(z))$ é€šå¸¸è¢«ç§°ä¸ºæ½œå˜é‡ $bold(z)$ çš„å…ˆéªŒåˆ†å¸ƒï¼Œå› ä¸ºå®ƒä¸ä»¥ä»»ä½•è§‚æµ‹æ•°æ®ä¸ºæ¡ä»¶ã€‚


== Intractabilities
#v(.9em)
dlvmä¸­æœ€å¤§ä¼¼ç„¶å­¦ä¹ çš„ä¸»è¦å›°éš¾æ˜¯*the marginal probability of data under the model*é€šå¸¸éš¾ä»¥å¤„ç†ã€‚

ç”±äºè®¡ç®—è¾¹é™…ä¼¼ç„¶(æˆ–æ¨¡å‹è¯æ®)çš„æ–¹ç¨‹ä¸­çš„ç§¯åˆ†ï¼Œ$p_theta (x) = integral p_Î¸ (x, z) dif z$ï¼Œ *not having an
analytic solution or efficient estimator.*ã€‚ç”±äºè¿™ç§ä¸å¯è§£æ€§ï¼Œæˆ‘ä»¬ä¸èƒ½å¯¹å…¶å‚æ•°è¿›è¡Œå¾®åˆ†å¹¶ä¼˜åŒ–å®ƒï¼Œå°±åƒæˆ‘ä»¬å¯¹å®Œå…¨è§‚æµ‹æ¨¡å‹æ‰€åšçš„é‚£æ ·ã€‚è®¡ç®—è¾¹ç¼˜ä¼¼ç„¶*Need to integrate high-dimensional space*ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯éå¸¸å›°éš¾çš„ã€‚æ‰€ä»¥åœ¨ MLE ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æœ€å¤§åŒ–æ•°æ®åœ¨æ¨¡å‹ä¸‹çš„æ¦‚ç‡ã€‚å¯¹äº DLVMsï¼Œè¿™æ¶‰åŠåˆ°è®¡ç®—è¾¹ç¼˜ä¼¼ç„¶ $p_theta (x)$ï¼Œä½†ç”±äºå…¶ä¸å¯è§£æ€§ï¼Œç›´æ¥è¿›è¡Œ MLE æ˜¯ä¸ç°å®çš„ã€‚ç”±äºç›´æ¥è®¡ç®—è¾¹ç¼˜ä¼¼ç„¶æ˜¯ä¸å¯è¡Œçš„ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨å˜åˆ†æ¨æ–­ï¼ˆ*Variational Inference*ï¼‰æˆ–é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ï¼ˆ*MCMC*ï¼‰æ–¹æ³•æ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒã€‚è¿™äº›æ–¹æ³•é€šè¿‡å¼•å…¥å¯è§£çš„è¿‘ä¼¼æ¥å¤„ç†ä¸å¯è§£çš„ç§¯åˆ†é—®é¢˜ã€‚åŒæ ·ï¼Œç¥ç»ç½‘ç»œå‚æ•°åŒ–çš„(æœ‰å‘æ¨¡å‹) $p(theta|cal(D))$ çš„åéªŒé€šå¸¸éš¾ä»¥ç²¾ç¡®è®¡ç®—ï¼Œéœ€è¦è¿‘ä¼¼æ¨ç†æŠ€æœ¯ã€‚


= Variational Autoencoders
#v(.9em)
== Encoder or Approximate Posterior
#v(.9em)
dlvmä¼°è®¡è¿™ç§æ¨¡å‹ä¸­çš„å¯¹æ•°ä¼¼ç„¶åˆ†å¸ƒå’ŒåéªŒåˆ†å¸ƒçš„é—®é¢˜ã€‚å˜åˆ†è‡ªç¼–ç å™¨(VAEs)æ¡†æ¶æä¾›äº†ä¸€ç§è®¡ç®—æ•ˆç‡é«˜çš„æ–¹æ³•æ¥ä¼˜åŒ–dlvmï¼Œå¹¶ç»“åˆç›¸åº”çš„æ¨ç†æ¨¡å‹ä½¿ç”¨SGDè¿›è¡Œä¼˜åŒ–ã€‚
ä¸ºäº†å°†DLVMçš„åéªŒæ¨ç†å’Œå­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºå¯å¤„ç†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå‚æ•°æ¨ç†æ¨¡å‹ $q_(phi.alt) (z|x)$ã€‚è¿™ä¸ªæ¨¡å‹ä¹Ÿè¢«ç§°ä¸ºç¼–ç å™¨æˆ–è¯†åˆ«æ¨¡å‹ã€‚ç”¨ $phi.alt$ è¡¨ç¤ºè¯¥æ¨ç†æ¨¡å‹çš„å‚æ•°ï¼Œä¹Ÿç§°ä¸ºå˜åˆ†å‚æ•°ã€‚æˆ‘ä»¬ä¼˜åŒ–å˜åˆ†å‚æ•° $phi.alt$:

$
q_(phi.alt) (bold(z)|bold(x)) approx p_theta (bold(z)|bold(x))
$
åƒDLVMä¸€æ ·ï¼Œæ¨ç†æ¨¡å‹å¯ä»¥æ˜¯(å‡ ä¹)ä»»ä½•æœ‰å‘å›¾å½¢æ¨¡å‹:
$
q_(phi.alt) (bold(z)|bold(x)) = q_(phi.alt) (bold(z)_1,...,bold(z)_M |bold(x)) = product_(j=1)^M q_(phi.alt) (bold(z)_j | P_a (bold(z)_j),bold(x)) 
$
$P_a (bold(z)_j)$ æ˜¯å˜é‡ $bold(z)_j$ åœ¨æœ‰å‘å›¾ä¸­çš„çˆ¶å˜é‡é›†åˆã€‚ä¸DLVMç±»ä¼¼ï¼Œåˆ†å¸ƒ $q_(phi.alt)(bold(z)|bold(x))$ å¯ä»¥ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œå‚æ•°åŒ–ã€‚

$
(mu,log sigma) = "EncoderNeuralNet"_(phi.alt) (x)
$

$
q_(phi.alt) (bold(z)|bold(x)) = cal(N) (z;mu,"diag"(sigma))
$
æˆ‘ä»¬ä½¿ç”¨å•ä¸ªç¼–ç å™¨ç¥ç»ç½‘ç»œå¯¹æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ•°æ®ç‚¹æ‰§è¡ŒåéªŒæ¨ç†ã€‚è¿™å¯ä»¥ä¸æ›´ä¼ ç»Ÿçš„å˜åˆ†æ¨ç†æ–¹æ³•å½¢æˆå¯¹æ¯”ï¼Œå…¶ä¸­å˜åˆ†å‚æ•°ä¸æ˜¯å…±äº«çš„ï¼Œè€Œæ˜¯æ¯ä¸ªæ•°æ®ç‚¹å•ç‹¬è¿­ä»£ä¼˜åŒ–çš„ã€‚é€šè¿‡å¹³æ‘Šæ¨ç†ï¼Œæˆ‘ä»¬å¯ä»¥é¿å…æ¯ä¸ªæ•°æ®ç‚¹çš„ä¼˜åŒ–å¾ªç¯ï¼Œå¹¶åˆ©ç”¨SGDçš„æ•ˆç‡ã€‚
#v(4em)
== Evidence Lower Bound (ELBO)
#v(1em)
#figure(
  image("../../img/vae.png", width:70%),
  caption: [
    It shows how to learn and generate new data through mapping between latent variable space ($cal(z)$-space) and observed data space ($cal(x)$-space).
  ],
) 
VAE é€šè¿‡ç¼–ç å™¨å’Œè§£ç å™¨ç½‘ç»œï¼Œåˆ©ç”¨å…ˆéªŒåˆ†å¸ƒã€åéªŒè¿‘ä¼¼å’Œé‡å»ºåˆ†å¸ƒï¼Œå®ç°å¯¹å¤æ‚æ•°æ®åˆ†å¸ƒçš„è¿‘ä¼¼å»ºæ¨¡å’Œç”Ÿæˆã€‚
#v(6em)
å˜åˆ†å‚æ•°$phi.alt$:
$
log p_theta (bold(x)) &= EE_(q_(phi.alt)(bold(z)|bold(x))) [log p_theta (x)]\
                 &= EE_(q_(phi.alt)(bold(z)|bold(x))) [log [(p_theta (bold(x),bold(z)))/ (p_theta (bold(z)|bold(x)))]]\
                 &= EE_(q_(phi.alt)(bold(z)|bold(x))) [log [(p_theta (bold(x),bold(z)))/ (q_(phi.alt) (bold(z)|bold(x))) (q_(phi.alt) (bold(z)|bold(x)))/(p_theta (bold(z)|bold(x)))]]\
                 &= underbrace(
                    EE_(q_(phi.alt)(bold(z)|bold(x))) [log [(p_theta (bold(x),bold(z)))/ (q_(phi.alt) (bold(z)|bold(x) ))]], 
                    
                    = cal(L)_theta.. phi.alt(x)\
                    ("ELBO")
                    ) 
                    +
                    underbrace(
                    EE_(q_(phi.alt)(bold(z)|bold(x))) [log [(q_(phi.alt) (bold(z)|bold(x)))/(p_theta (bold(z)|bold(x))) ]],
                    =D_"KL" (q_(phi.alt) (bold(z)|bold(x))||p_theta (bold(z)|bold(x)) )
                    )
$
ç¬¬äºŒé¡¹æ˜¯$q_(phi.alt) (z|x)$ä¸$p_Î¸ (z|x)$ä¹‹é—´çš„*Kullback-Leibler (KL)*æ•£åº¦æ˜¯éè´Ÿçš„,å½“ç­‰äº0æ—¶,$q_(phi.alt)(z|x)$ ç­‰äºçœŸå®åéªŒåˆ†å¸ƒ:
$
D_"KL" (q_(phi.alt) (bold(z)|bold(x))||p_theta (bold(z)|bold(x)) ) >= 0
$

ç¬¬ä¸€é¡¹æ˜¯å˜åˆ†ä¸‹ç•Œï¼Œä¹Ÿç§°ä¸ºè¯æ®ä¸‹ç•Œ(ELBO):
$
cal(L)_(theta,phi.alt) (bold(x)) = EE_(q_(phi.alt)(bold(z|bold(x)))) [log p _theta (bold(x),bold(z)) - log q_(phi.alt) (bold(z)|bold(x))]
$
ç”±äºKLæ•£åº¦çš„éè´Ÿæ€§ï¼ŒELBOæ˜¯æ•°æ®çš„å¯¹æ•°ä¼¼ç„¶çš„ä¸‹ç•Œ:
$
cal(L)_(theta,phi.alt) (bold(x)) &= log p_theta (bold(x)) -D_"KL" (q_(phi.alt) (bold(z)|bold(x))||p_theta (bold(z)|bold(x)) )\
&<=log p_theta (bold(x)) 
$



== Stochastic Gradient-Based Optimization of the ELBO
#v(.9em)
ELBOçš„ä¸€ä¸ªé‡è¦æ€§è´¨æ˜¯ï¼Œå®ƒå…è®¸ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™(SGD)å¯¹æ‰€æœ‰å‚æ•°( $phi.alt$ å’Œ $theta$ )è¿›è¡Œè”åˆä¼˜åŒ–ã€‚

æˆ‘ä»¬å¯ä»¥ä»$phi.alt$å’Œ$theta$çš„éšæœºåˆå§‹å€¼å¼€å§‹ï¼Œéšæœºä¼˜åŒ–å®ƒä»¬çš„å€¼ï¼Œç›´åˆ°æ”¶æ•›ã€‚

$
cal( L ) _ ( theta, phi.alt ) ( cal( D ) ) = sum _ ( bold( upright( x ) ) in cal( D ) ) cal( L ) _ ( theta, phi.alt ) ( bold( upright( x ) ) )
$

ä¸€èˆ¬æ¥è¯´ï¼Œå•ä¸ªæ•°æ®ç‚¹ELBOåŠå…¶æ¢¯åº¦ $nabla_(theta,phi.alt) cal(L)_(theta,phi.alt)(x)$ æ˜¯éš¾ä»¥å¤„ç†çš„ã€‚ç„¶è€Œï¼Œæ­£å¦‚æˆ‘ä»¬å°†å±•ç¤ºçš„é‚£æ ·ï¼Œå­˜åœ¨è‰¯å¥½çš„æ— åä¼°è®¡é‡$tilde(nabla)_(theta,phi.alt)cal(L)_(theta,phi.alt)(x) $ï¼Œè¿™æ ·æˆ‘ä»¬ä»ç„¶å¯ä»¥æ‰§è¡Œå°æ‰¹é‡SGDã€‚ç”Ÿæˆæ¨¡å‹å‚æ•°$theta$ä¸‹ELBOçš„æ— åæ¢¯åº¦å¾ˆå®¹æ˜“å¾—åˆ°:

$
nabla _ ( theta ) cal( L ) _ ( theta ) cal( L ) _ ( theta, phi.alt ) ( bold( upright( x ) ) ) 
    & = nabla _ ( theta ) bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) ) ) [ log p _ ( theta ) ( bold( upright( z ) ), bold( upright( z ) ) ) ] \ 
    & = bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( z ) ) ) ) [ nabla _ ( theta ) ( log p _ ( theta ) ( bold( upright( x ) ), bold( upright( z ) ) ) - log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( z ) ) ) ) ] \ 
    & "Monte Carlo"\
    & tilde.eq nabla _ ( theta ) ( log p _ ( theta ) ( bold( upright( x ) ), bold( upright( z ) ) ) - log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) ) \ 
    & = nabla _ ( theta ) ( log p _ ( theta ) ( bold( upright( x ) ), bold( upright( z ) ) )
$

== Reparameterization Trick
#v(.9em)
å¯¹äºè¿ç»­æ½œå˜é‡å’Œå¯å¾®ç¼–ç å™¨å’Œç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡å˜é‡çš„å˜åŒ–ç›´æ¥å¯¹ELBOè¿›è¡Œ$phi.alt$å’Œ$theta$çš„å¾®åˆ†ï¼Œä¹Ÿç§°ä¸ºé‡å‚æ•°åŒ–æŠ€å·§ã€‚

=== Change of variables
#v(.9em)
é¦–å…ˆï¼Œæˆ‘ä»¬å°†éšæœºå˜é‡ $bold( upright( z ) ) tilde q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) $è¡¨ç¤ºä¸ºå¦ä¸€ä¸ªéšæœºå˜é‡ $ epsilon.alt$ çš„å¯å¾®(å¯é€†)å˜æ¢ï¼Œç»™å®š$z$å’Œ$phi.alt$:
$
bold( upright( z ) ) = g ( epsilon.alt, phi.alt, bold( upright( x ) ) )
$
å…¶ä¸­ï¼Œéšæœºå˜é‡ $Ïµ$ çš„åˆ†å¸ƒä¸ $x$ æˆ– $ğœ™$ æ— å…³ã€‚è¿™ä¸€æ­¥çš„ç›®çš„æ˜¯å°†éšæœºå˜é‡ $cal(z)$ è¡¨ç¤ºä¸ºä¸€ç§å¯å¾®åˆ†çš„æ–¹å¼ï¼Œä»è€Œä½¿å¾—æ¢¯åº¦å¯ä»¥æœ‰æ•ˆåœ°è®¡ç®—ã€‚

é‡æ–°å‚æ•°åŒ–æŠ€å·§çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒå°†ä¸å¯å¾®åˆ†çš„é‡‡æ ·è¿‡ç¨‹è½¬åŒ–ä¸ºå¯å¾®åˆ†çš„å‚æ•°åŒ–è¿‡ç¨‹ï¼Œä»è€Œä½¿å¾—æ¢¯åº¦ä¸‹é™ä¼˜åŒ–æˆä¸ºå¯èƒ½ã€‚

#block(
  width: 100%,
  fill: white,
  inset: 8pt,
  stroke: 0.5pt,
  [
    *Algorithm 1*: 
    ELBOçš„éšæœºä¼˜åŒ–ã€‚ç”±äºå™ªå£°æ¥æºäºå°æ‰¹é‡é‡‡æ ·å’Œ$p(epsilon)$çš„é‡‡æ ·ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªåŒé‡éšæœºä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬ä¹ŸæŠŠè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºè‡ªåŠ¨ç¼–ç å˜åˆ†è´å¶æ–¯(AEVB)_the Auto-Encoding Variational Bayes_ç®—æ³•ã€‚

    *Data:*
    - $cal(D)$: Dataset
    - $q_phi.alt (z|x)$: Inference model
    - $p_theta (x,z)$: Generative model

    *Result:*
    - $theta, phi.alt$: Learned parameters

    $(theta, phi.alt) arrow.l$ Initialize parameters \
    *while* SGD not converged *do* \
    $quad cal(M) tilde cal(D)$ (Random minibatch of data) \
    $quad epsilon tilde p(epsilon)$ (Random noise for every datapoint in $cal(M)$) \
    $quad$ Compute $tilde(cal(L))_(theta,phi.alt)(cal(M),epsilon)$ and its gradients $nabla_(theta,phi.alt) tilde(cal(L))_(theta,phi.alt)(cal(M),epsilon)$ \
    $quad$ Update $theta$ and $phi.alt$ using SGD optimizer \
    *end*
  ]
)

=== Gradient of expectation under change of variable
#v(1em)
åœ¨å˜é‡å˜æ¢çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†æœŸæœ›ç”¨æ–°çš„éšæœºå˜é‡ $epsilon$ è¡¨ç¤ºï¼š
$
bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) ) ) [ f ( bold( upright( z ) ) ) ] = bb( E ) _ ( p ( epsilon.alt ) ) [ f ( bold( upright( z ) ) ) ]
$

å…¶ä¸­ï¼Œ$bold(z)=g(epsilon,phi.alt,bold(x))$ã€‚é€šè¿‡è¿™ç§å˜æ¢ï¼ŒæœŸæœ›è¿ç®—ç¬¦å’Œæ¢¯åº¦è¿ç®—ç¬¦å˜å¾—å¯äº¤æ¢ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œç®€å•çš„è’™ç‰¹å¡ç½—ä¼°è®¡:

$
nabla _ ( phi.alt ) bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) ) ( bold( upright( z ) ) ) ) [ f ( bold( upright( z ) ) ) ] 
    & = nabla _ ( phi.alt ) bb( E ) _ ( p ( epsilon.alt ) ) [ f ( bold( upright( z ) ) ) ] 
$
ç”±äº $z$ æ˜¯ $epsilon, phi, x$ çš„å¯å¾®å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥äº¤æ¢æ¢¯åº¦è¿ç®—ç¬¦å’ŒæœŸæœ›è¿ç®—ç¬¦,ç„¶åé€šè¿‡ä» $p(epsilon)$ ä¸­é‡‡æ · $epsilon$ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°å¯¹æ¢¯åº¦çš„è¿‘ä¼¼ä¼°è®¡ï¼š
$
nabla _ ( phi.alt ) bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) ) ( bold( upright( z ) ) ) ) [ f ( bold( upright( z ) ) ) ] 
    & = bb( E ) _ ( p ( epsilon.alt ) ) [ nabla _ ( phi.alt ) f ( bold( upright( z ) ) ) ] \ 
    & tilde.eq nabla _ ( phi.alt ) f ( bold( upright( z ) ) )
$


#figure(
  image("../../img/epsilon.png", width:100%),
  caption: [This image illustrates the reparameterization trick, a critical technique in variational autoencoders (VAEs) for efficient gradient-based optimization. The trick allows us to rewrite the sampling process of latent variables in a differentiable manner.
    .
  ],
) 

#let body-text = 10pt
#let summary-text = 11pt

å›¾ 2 å±•ç¤ºäº†é‡æ–°å‚æ•°åŒ–æŠ€å·§çš„å·¥ä½œåŸç†ï¼Œä¸»è¦åŒ…æ‹¬ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†ï¼šåŸå§‹å½¢å¼å’Œé‡æ–°å‚æ•°åŒ–å½¢å¼ã€‚
#grid(
  columns: (5fr, 5fr),
  gutter: 16pt,

  column-gutter: 0fr,
  [
    #text(size: body-text)[
      #set list(marker: text(green)[#sym.bullet])
      å·¦å›¾ï¼šåŸå§‹å½¢å¼ï¼ˆOriginal formï¼‰
      + èŠ‚ç‚¹è¡¨ç¤ºï¼š
        - *ç°è‰²èŠ‚ç‚¹ï¼ˆDeterministic nodeï¼‰*ï¼šè¡¨ç¤ºç¡®å®šæ€§èŠ‚ç‚¹ï¼Œå¦‚ç›®æ ‡å‡½æ•° $f$ã€‚
        - *è“è‰²èŠ‚ç‚¹ï¼ˆRandom nodeï¼‰*ï¼šè¡¨ç¤ºéšæœºèŠ‚ç‚¹ï¼Œå¦‚æ½œå˜é‡ $z$ã€‚

      + å›¾ç¤ºæè¿°ï¼š
        - éšæœºå˜é‡ $z$ ä»æ¨æ–­æ¨¡å‹ $q_phi.alt (z|x)$ ä¸­é‡‡æ ·ã€‚
        - ç›®æ ‡å‡½æ•° $f$ ä¾èµ–äº $z$ å’Œå‚æ•° $phi.alt$ã€‚

      2. é—®é¢˜ï¼š
        - æˆ‘ä»¬å¸Œæœ›å¯¹ç›®æ ‡å‡½æ•° $f$ æ±‚æ¢¯åº¦ä»¥ä¼˜åŒ–å‚æ•° $phi.alt$ã€‚
        - ç”±äº $z$ æ˜¯ä» $q_phi.alt (z|x)$ ä¸­é‡‡æ ·çš„ï¼Œæ— æ³•ç›´æ¥å¯¹ $z$ è¿›è¡Œåå‘ä¼ æ’­ï¼Œä»è€Œæ— æ³•å¯¹ $phi$ æ±‚å¯¼ã€‚

    ]
  ],

  [
    #text(size: body-text)[
      #set list(marker: text(blue)[â†’])
      å³å›¾ï¼šé‡æ–°å‚æ•°åŒ–å½¢å¼ï¼ˆReparameterized formï¼‰

      1. èŠ‚ç‚¹è¡¨ç¤ºï¼š
        - *ç°è‰²èŠ‚ç‚¹ï¼ˆDeterministic nodeï¼‰*ï¼šè¡¨ç¤ºç¡®å®šæ€§èŠ‚ç‚¹ï¼Œå¦‚ç›®æ ‡å‡½æ•° $f$ã€‚
        - *è“è‰²èŠ‚ç‚¹ï¼ˆRandom nodeï¼‰*ï¼šè¡¨ç¤ºéšæœºèŠ‚ç‚¹ï¼Œå¦‚å™ªå£° $epsilon$ã€‚

      2. å›¾ç¤ºæè¿°ï¼š
        - å°†éšæœºå˜é‡ $z$ è¡¨ç¤ºä¸º $epsilon, phi.alt, x$ çš„å¯å¾®å‡½æ•°ï¼š $z = g(phi.alt, x, epsilon)$ã€‚
        - $epsilon$ æ˜¯ä»ç®€å•çš„åˆ†å¸ƒ $p(epsilon)$ ä¸­é‡‡æ ·çš„éšæœºå™ªå£°ã€‚

      3. æ¢¯åº¦è®¡ç®—ï¼š
        - ç”±äº $z$ ç°åœ¨æ˜¯ $epsilon, phi.alt, x$ çš„å¯å¾®å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹ $z$ è¿›è¡Œåå‘ä¼ æ’­ã€‚
        - è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥å¯¹å‚æ•° $phi.alt$ æ±‚å¯¼å¹¶ä¼˜åŒ–ç›®æ ‡å‡½æ•° $f$ã€‚
    ]
  ]
)

é‡æ–°å‚æ•°åŒ–æŠ€å·§çš„æ­¥éª¤

  1. å˜é‡å˜æ¢ï¼šå°†æ½œå˜é‡ $z$ è¡¨ç¤ºä¸ºå™ªå£° $epsilon$ å’Œå‚æ•° $phi.alt$ã€è§‚æµ‹æ•°æ® $x$ çš„å¯å¾®å‡½æ•°ï¼š$z = g(epsilon, phi.alt,x)$ã€‚
  2. æœŸæœ›é‡å†™ï¼šåˆ©ç”¨å˜é‡å˜æ¢ï¼Œå°†æœŸæœ›ç”¨æ–°çš„éšæœºå˜é‡ $epsilon$ è¡¨ç¤ºï¼š$EE_q_phi.alt(z|x) [f(z)] = EE_p(epsilon) [f(z)]$ã€‚
  3. æ¢¯åº¦è®¡ç®—ï¼šäº¤æ¢æ¢¯åº¦è¿ç®—ç¬¦å’ŒæœŸæœ›è¿ç®—ç¬¦ï¼Œä½¿ç”¨è’™ç‰¹å¡ç½—é‡‡æ ·è¿‘ä¼¼æœŸæœ›ï¼Œä»è€Œè®¡ç®—æ¢¯åº¦ã€‚

é‡æ–°å‚æ•°åŒ–æŠ€å·§çš„ä¼˜åŠ¿

- ä½¿æ¢¯åº¦è®¡ç®—å¯è¡Œï¼šé€šè¿‡å°†é‡‡æ ·è¿‡ç¨‹å¤–éƒ¨åŒ–ï¼Œä½¿å¾—æ¢¯åº¦å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è¿›è¡Œè®¡ç®—ã€‚
- æé«˜è®¡ç®—æ•ˆç‡ï¼šç®€åŒ–æ¢¯åº¦è®¡ç®—è¿‡ç¨‹ï¼Œä½¿ç”¨è’™ç‰¹å¡ç½—é‡‡æ ·è¿›è¡Œè¿‘ä¼¼ä¼°è®¡ã€‚

=== Gradient of ELBO
#v(.9em)
åœ¨é‡æ–°å‚æ•°åŒ–çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æ›¿æ¢æœŸæœ› $q_phi.alt (z|x)$ä¸ä¸€ä¸ª$p (epsilon)$ã€‚ELBOå¯ä»¥é‡å†™ä¸º:
$
cal( L ) _ ( theta, phi.alt ) ( x ) & = bb( E ) _ ( q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) ) [ log p _ ( theta ) ( bold( upright( x ) ), bold( upright( z ) ) ) - log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( z ) ) ) ] \ & = bb( E ) _ ( p ( epsilon.alt ) ) [ log p _ ( theta ) ( bold( upright( x ) ), bold( upright( z ) ) ) - log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) ]
$
å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å½¢æˆå•ä¸ªæ•°æ®ç‚¹ELBOçš„ç®€å•è’™ç‰¹å¡ç½—ä¼°è®¡é‡$tilde( cal( L ) ) _ ( theta, phi.alt ) ( x )$ï¼Œå…¶ä¸­æˆ‘ä»¬ä½¿ç”¨æ¥è‡ª$p(epsilon)$çš„å•ä¸ªå™ªå£°æ ·æœ¬$epsilon$:
$
epsilon.alt tilde p ( epsilon.alt ) \ z = g ( phi.alt, x, epsilon.alt ) \ tilde( L ) _ ( theta, phi.alt ) ( x ) = log p _ ( theta ) ( x, z ) - log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) )
$
è¿™ä¸€ç³»åˆ—æ“ä½œå¯ä»¥åœ¨TensorFlowç­‰è½¯ä»¶ä¸­è¡¨ç¤ºä¸ºç¬¦å·å›¾ï¼Œå¹¶æ¯«ä¸è´¹åŠ›åœ°å¾®åˆ†å‚æ•°Î¸å’ŒÏ†.

è¯¥ç®—æ³•æœ€åˆè¢«ç§°ä¸º_Auto-Encoding Variational Bayes _(AEVB)ç®—æ³•ã€‚æ›´ä¸€èˆ¬åœ°è¯´ï¼Œé‡æ–°å‚æ•°åŒ–çš„ELBOä¼°è®¡è¢«ç§°ä¸ºéšæœºæ¢¯åº¦å˜åˆ†è´å¶æ–¯_(SGVB)_ä¼°è®¡ã€‚è¿™ä¸ªä¼°è®¡å™¨ä¹Ÿå¯ä»¥ç”¨æ¥ä¼°è®¡æ¨¡å‹å‚æ•°çš„åéªŒ.

=== Computation of log qÏ†(z|x)
#v(.9em)
ELBO(ä¼°è®¡é‡)çš„è®¡ç®—éœ€è¦è®¡ç®—å¯†åº¦å¯¹æ•°$q_phi.alt (z|x)$ï¼Œç»™å®šå€¼$x$ï¼Œå¹¶ç»™å®šå€¼$z$æˆ–ç­‰ä»·çš„$epsilon$ã€‚è¿™ä¸ªå¯¹æ•°å¯†åº¦æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—ï¼Œåªè¦æˆ‘ä»¬é€‰æ‹©æ­£ç¡®çš„å˜æ¢$g()$ã€‚

æ³¨æ„ï¼Œæˆ‘ä»¬é€šå¸¸çŸ¥é“å¯†åº¦$p(epsilon)$ï¼Œå› ä¸ºè¿™æ˜¯æ‰€é€‰å™ªå£°åˆ†å¸ƒçš„å¯†åº¦ã€‚åªè¦$g(.)$æ˜¯å¯é€†å‡½æ•°ï¼Œåˆ™$epsilon$å’Œ$z$çš„å¯†åº¦å…³ç³»å¼ä¸º:

(é€šè¿‡å˜é‡å˜æ¢å’Œæ¦‚ç‡å¯†åº¦å‡½æ•°çš„æ€§è´¨æ¨å¯¼å‡ºæ¥çš„,è§2.4.4.1)
$
log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = log p ( epsilon.alt ) - log d _ ( phi.alt ) ( bold( upright( x ) ), epsilon.alt )
$
å…¶ä¸­ç¬¬äºŒé¡¹æ˜¯é›…å¯æ¯”çŸ©é˜µ$(diff_z/diff_epsilon)$çš„è¡Œåˆ—å¼ç»å¯¹å€¼çš„å¯¹æ•°,åŒ…å«äº†ä» $epsilon$ åˆ° $cal(z)$ å˜æ¢çš„æ‰€æœ‰ä¸€é˜¶å¯¼æ•°:
$
log d _ ( phi.alt ) ( bold( upright( x ) ), epsilon.alt ) = log abs( det ( ( diff bold( upright( z ) ) ) / ( diff epsilon.alt ) ) )
$



$
( diff bold( upright( z ) ) ) / ( diff epsilon.alt ) = ( diff ( z _ ( 1 ),..., z _ ( k ) ) ) / ( diff ( epsilon.alt _ ( 1 ),..., epsilon.alt _ ( k ) ) ) = mat( ( diff z _ ( 1 ) ) / ( diff epsilon.alt _ ( 1 ) ),..., ( diff z _ ( 1 ) ) / ( diff epsilon.alt _ ( k ) ) ; dots.v, dots.down, dots.v ; ( diff z _ ( k ) ) / ( diff epsilon.alt _ ( 1 ) ),..., ( diff z _ ( k ) ) / ( diff epsilon.alt _ ( k ) ) )
$


==== deduce
#v(.9em)
å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªéšæœºå˜é‡ $z$ï¼Œå®ƒé€šè¿‡ä¸€ä¸ªå¯å¾®ä¸”å¯é€†çš„å˜æ¢ $g$ ç”±éšæœºå˜é‡ $Ïµ$ å¾—åˆ°ï¼Œå³ï¼š
$
bold( upright( z ) ) = g ( epsilon.alt, phi.alt, bold( upright( x ) ) )
$
æ ¹æ®æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å˜æ¢æ€§è´¨ï¼Œå¦‚æœ $z=g(epsilon)$ï¼Œé‚£ä¹ˆ $z$ çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $q_phi.alt (z|x)$ å’Œ $epsilon$ çš„æ¦‚ç‡å¯†åº¦å‡½æ•° $p(epsilon)$ ä¹‹é—´æœ‰å¦‚ä¸‹å…³ç³»ï¼š
$
q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = p ( epsilon.alt ) abs( det ( ( diff epsilon.alt ) / ( diff bold( upright( z ) ) ) ) )
$

ä½†æ˜¯ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸æ˜¯çŸ¥é“ $(diff z)/(diff epsilon)$ è€Œä¸æ˜¯ $(diff epsilon)/(diff z)$ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨é›…å¯æ¯”è¡Œåˆ—å¼çš„é€†å…³ç³»ï¼š
$
abs( det ( ( diff epsilon.alt ) / ( diff z ) ) ) = ( 1 ) / ( abs( det ( ( diff z ) / ( diff epsilon.alt ) ) ) )
$
å¯†åº¦å…³ç³»è¡¨ç¤º:
$
q _ ( phi.alt ) ( bold( upright( z ) ) )  = p ( epsilon.alt ) abs( det ( ( diff epsilon.alt ) ( diff z ) ) ) = p ( epsilon.alt ) abs( ( 1 ) / ( det ( ( diff epsilon.alt ) / ( diff epsilon.alt ) ) ) ) 
$
$ 
q _ ( phi.alt ) ( bold( upright( z ) ) ) & = p ( epsilon.alt ) abs( det ( ( diff z ) / ( diff epsilon.alt ) ) ) ^ ( - 1 )
$

== Factorized Gaussian posteriors
#v(.9em)
ä¸€ä¸ªå¸¸è§çš„é€‰æ‹©æ˜¯ä¸€ä¸ªç®€å•çš„_ factorized Gaussian encoder_
$
q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = cal( N ) ( bold( upright( z ) ) ; mu, op( upright( d i a g ) ) ( sigma ^ ( 2 ) ) )
$
å…¶ä¸­ï¼Œ$mu$ å’Œ $log sigma$ é€šè¿‡ä¸€ä¸ªç¼–ç å™¨ç¥ç»ç½‘ç»œ 
$"EncoderNeuralNet"_phi.alt (x)$è·å¾—ï¼š
$
 q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = cal( N ) ( bold( upright( z ) ) ; mu, op( upright( d i a g ) ) ( sigma ^ ( 2 ) ) )
$
åéªŒåˆ†å¸ƒå› å­åŒ–ä¸ºæ¯ä¸ªæ½œåœ¨å˜é‡ $cal(z)_i$  çš„é«˜æ–¯åˆ†å¸ƒçš„ä¹˜ç§¯ï¼š
$
q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = product _ ( i ) q _ ( phi.alt ) ( z _ ( i ) | bold( upright( x ) ) ) = product _ ( i ) cal( N ) ( z _ ( i ) ; mu _ ( i ), sigma _ ( i ) ^ ( 2 ) )
$
å°†é«˜æ–¯éšæœºå˜é‡ $z$ è¡¨ç¤ºä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒ $epsilon$ ç»è¿‡çº¿æ€§å˜æ¢çš„ç»“æœï¼š
$
epsilon.alt tilde cal( N ) ( 0, bold( upright( I ) ) ) 
$
$ 
( mu, log sigma ) = E n c o d e r N e u r a l N e t _ ( phi.alt ) ( bold( upright( x ) ) ) 
$
$ 
bold( upright( z ) ) = mu + sigma dot.circle epsilon.alt
$
ä» $Ïµ$ åˆ° $z$ çš„å˜æ¢çš„é›…å¯æ¯”çŸ©é˜µæ˜¯å¯¹è§’çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿å…ƒç´ æ˜¯ $Ïƒ$:
$
(diff z)/(diff epsilon) = d i a g (sigma)
$
å¯¹è§’çŸ©é˜µçš„è¡Œåˆ—å¼æ˜¯å…¶å¯¹è§’çº¿å…ƒç´ çš„ä¹˜ç§¯ï¼Œå› æ­¤å…¶å¯¹æ•°è¡Œåˆ—å¼æ˜¯å¯¹è§’çº¿å…ƒç´ çš„å¯¹æ•°ä¹‹å’Œï¼š
$
log d _ ( phi.alt ) ( bold( upright( x ) ), epsilon.alt ) = log abs( det ( ( diff bold( upright( z ) ) ) / ( diff epsilon.alt ) ) ) = sum _ ( i )  log sigma _ ( i )
$
åéªŒå¯†åº¦ä¸º:
$
log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) & = log p ( epsilon.alt ) - log d _ ( phi.alt ) ( bold( upright( x ) ), epsilon.alt ) \ & = sum _ ( i ) log cal( N ) ( epsilon.alt _ ( i ) ; 0, 1 ) - log sigma _ ( i )
$

=== Full-covariance Gaussian posterior
#v(.9em)
å› å¼é«˜æ–¯åéªŒå¯ä»¥æ¨å¹¿ä¸ºå…·æœ‰å…¨åæ–¹å·®çš„é«˜æ–¯:
$
q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = cal( N ) ( bold( upright( z ) ) ; bold( upright( mu ) ), bold( upright( Sigma ) ) )
$
è¯¥åˆ†å¸ƒçš„é‡æ–°å‚æ•°åŒ–ç”±ä¸‹å¼ç»™å‡º:
$
epsilon.alt tilde cal( N ) ( 0, bold( upright( I ) ) ) \ bold( upright( z ) ) = mu + bold( upright( L ) ) epsilon.alt
$
å…¶ä¸­ $bold(L)$ æ˜¯ä¸‹(æˆ–ä¸Š)ä¸‰è§’çŸ©é˜µï¼Œåœ¨å¯¹è§’çº¿ä¸Šæœ‰éé›¶å…ƒç´ ã€‚éå¯¹è§’çº¿å…ƒç´ å®šä¹‰äº† $z$ ä¸­å…ƒç´ çš„ç›¸å…³æ€§(åæ–¹å·®)ã€‚å¯¹äºè¿™ç§å…¨åæ–¹å·®é«˜æ–¯æ¨¡å‹ï¼Œé›…å¯æ¯”è¡Œåˆ—å¼å¾ˆç®€å•ï¼Œå› ä¸ºï¼š
$
(diff z)/(diff epsilon) = bold(L)
$
å› ä¸º $bold(L)$ æ˜¯ä¸‹(æˆ–ä¸Š)ä¸‰è§’çŸ©é˜µï¼Œæ‰€ä»¥è¡Œåˆ—å¼æ˜¯å…¶å¯¹è§’çº¿å…ƒç´ çš„ä¹˜ç§¯:
$
log abs( det ( ( diff bold( upright( z ) ) ) / ( diff epsilon.alt ) ) ) = sum _ ( i ) log abs( L _ ( i i ) )
$
åéªŒå¯†åº¦çš„å¯¹æ•°è®¡ç®—ä¸ºï¼š
$
log q _ ( phi.alt ) ( bold( upright( z ) ) | bold( upright( x ) ) ) = log p ( epsilon.alt ) - sum _ ( i ) log abs( L _ ( i i ) )
$
åæ–¹å·®çŸ©é˜µ $Î£$ å¯ä»¥é€šè¿‡ Cholesky åˆ†è§£å¾—åˆ°ï¼š
$
Sigma = bold( upright( L ) ) bold( upright( L ) ) ^ ( T )
$
åæ–¹å·®çŸ©é˜µ $sum$ çš„è®¡ç®—è¿‡ç¨‹ä¸ºï¼š
$
Sigma = bb( E ) [ ( bold( upright( z ) ) - bb( E ) [ bold( upright( z ) ) ] ) ( bold( upright( z ) ) - bb( E ) [ bold( upright( z ) ) ] ) ^ ( T ) ]
$
é€šè¿‡å¼•å…¥ $epsilon$ï¼Œæˆ‘ä»¬æœ‰:
$
Sigma = bb( E ) [ ( bold( upright( z ) ) - bb( E ) [ bold( upright( z ) ) ] ) ( bold( upright( z ) ) - bb( E ) [ bold( upright( z ) ) ] ) ^ ( T ) ]
$
ç”±äº $epsilon.alt tilde cal( N ) ( 0, bold( upright( I ) ) )$ æœ‰ï¼š
$
bb( E ) [ epsilon.alt epsilon.alt ^ ( T ) ] = bold( upright( I ) )
$
é€šè¿‡ç¥ç»ç½‘ç»œè·å–å‚æ•° $mu$ ,$log sigma$å’Œ $bold(L)'$ ï¼š
$
( mu, log sigma, bold( upright( L )' ) )  <- E n c o d e r N e u r "Net" _ ( phi.alt ) ( bold( upright( x ) ) ) 
$
æ„å»º$bold(L)$çŸ©é˜µ:
$
L <- bold( upright( L ) ) _ ( m a s k ) dot.circle bold( upright( L ) ) ^ ( zwj' ) + "diag" ( sigma )
$
$bold(L)_(m a s k)$æ˜¯ä¸€ä¸ªæ©ç çŸ©é˜µï¼Œå…¶å¯¹è§’çº¿ä¸‹æ–¹çš„å…ƒç´ ä¸º1ï¼Œå¯¹è§’çº¿å’Œå¯¹è§’çº¿ä¸Šæ–¹çš„å…ƒç´ ä¸º0ã€‚

å¯¹äºå› å­åŒ–é«˜æ–¯æƒ…å½¢ï¼ŒåéªŒå¯†åº¦çš„å¯¹æ•°è®¡ç®—ä¸ºï¼š
$
log abs( det ( ( diff bold( upright( z ) ) ) / ( diff epsilon.alt ) ) ) = sum _ ( i ) log sigma _ ( i )
$

#block(
  width: 100%,
  fill: white,
  inset: 8pt,
  stroke: 0.5pt,
  [
    *Algorithm 2*: 
    å•æ•°æ®ç‚¹ELBOæ— åä¼°è®¡çš„è®¡ç®—ï¼Œç”¨äºå…·æœ‰å…¨åæ–¹å·®é«˜æ–¯æ¨ç†æ¨¡å‹å’Œå› å­åŒ–ä¼¯åŠªåˆ©ç”Ÿæˆæ¨¡å‹çš„VAEç¤ºä¾‹ã€‚$L_("mask")$æ˜¯ä¸€ä¸ªæ©è”½çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸ŠåŠä»¥ä¸Šä¸ºé›¶ï¼Œå¯¹è§’çº¿ä¸‹ä¸ºä¸€ã€‚

    *Data:*
    - $x$: : a datapoint, and optionally other conditioning information
    - $epsilon$: a random sample from $p(epsilon) = cal(N)(0,I)$ 
    - $theta$:  Generative model parameters
    - $phi.alt$: Inference model parameters
    - $q_phi.alt(z|x)$: Inference model
    - $p_theta(x,z)$: Generative model


    *Result:*
    - $tilde(cal(L))$: å•æ•°æ®ç‚¹ELBO $cal(L)_(theta,phi.alt)(x)$ çš„æ— åä¼°è®¡
    #v(.9em)
    $(mu, log sigma, bold(L')) arrow.l "EncoderNeuralNet"_phi.alt (x)$ \
    $bold(L) arrow.l bold(L)_(m a s k) dot.circle bold(L') + d i a g (sigma)$ \
    $epsilon tilde cal(N)(0,I)$ \
    $z arrow.l bold(L) epsilon + mu$ \
    $tilde(cal(L))_("logqz") arrow.l - sum_i (1/2(epsilon_i^2 + log(2pi) + log sigma_i))_i$
    #h(1fr) 
    $triangle.r = q_phi.alt (z|x)$ \
    $tilde(cal(L))_("logpz") arrow.l - sum_i (1/2(z_i^2 + log(2pi)))$
    #h(1fr) 
    $triangle.r = p_theta (z)$ \
    $p arrow.l "DecoderNeuralNet"_theta (z)$ \
    $tilde(cal(L))_("logpx") arrow.l sum_i (x_i log p_i + (1 - x_i) log(1 - p_i))$ 
    #h(1fr) 
    $triangle.r = p_theta (x|z)$ \
    $tilde(cal(L)) = tilde(cal(L))_("logpx") + tilde(cal(L))_("logpz") - tilde(cal(L))_("logqz")$
  ]
)




